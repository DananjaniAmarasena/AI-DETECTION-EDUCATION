{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97a4f97-f560-4935-b6eb-a1f7484ee23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/981.5 kB 217.9 kB/s eta 0:00:05\n",
      "     - ----------------------------------- 30.7/981.5 kB 217.9 kB/s eta 0:00:05\n",
      "     -- ---------------------------------- 61.4/981.5 kB 297.7 kB/s eta 0:00:04\n",
      "     ---- ------------------------------- 112.6/981.5 kB 467.6 kB/s eta 0:00:02\n",
      "     ---- ------------------------------- 122.9/981.5 kB 481.4 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 225.3/981.5 kB 689.2 kB/s eta 0:00:02\n",
      "     ----------- ------------------------ 307.2/981.5 kB 827.2 kB/s eta 0:00:01\n",
      "     ------------- ---------------------- 368.6/981.5 kB 919.0 kB/s eta 0:00:01\n",
      "     --------------- -------------------- 419.8/981.5 kB 937.3 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 501.8/981.5 kB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 583.7/981.5 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 634.9/981.5 kB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------- --------- 737.3/981.5 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 839.7/981.5 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 870.4/981.5 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 952.3/981.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------  972.8/981.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------  972.8/981.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 1.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\muditha\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993255 sha256=26d340651d2230ee7bb1791eb696e21550e8c50de4d7531c773cd58999d67fc5\n",
      "  Stored in directory: c:\\users\\muditha\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "#install packege for language detection\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f862f6f-4f04-4c31-b988-2305ecc0dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdc7d774-fe62-43ea-b6bd-e8e55991ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muditha\\AppData\\Local\\Temp\\ipykernel_31944\\1304483620.py:2: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/Muditha/OneDrive - University of Eastern Finland/Documents/UEF/Thesis/AI-writing-detector/data/rawdataset.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before cleaning: 38123\n",
      "After dropping missing abstracts: 29809\n",
      "After filtering English Abstracts: 28168\n",
      "After removing garbage/irregular abstracts: 27884\n",
      "After filtering by valid year range: 27649\n",
      "After removing duplicate abstracts: 27048\n"
     ]
    }
   ],
   "source": [
    "#load raw dataset\n",
    "df = pd.read_csv('C:/Users/Muditha/OneDrive - University of Eastern Finland/Documents/UEF/Thesis/AI-writing-detector/data/rawdataset.csv')\n",
    "print(\"Total records before cleaning:\", len(df))\n",
    "\n",
    "#drop missing abstracts\n",
    "df = df.dropna(subset=['Abstract'])\n",
    "print(\"After dropping missing abstracts:\", len(df))\n",
    "\n",
    "#detect and keep abstracts only written in English language\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "            return False\n",
    "\n",
    "df['is_english'] = df['Abstract'].apply(is_english)\n",
    "df = df[df['is_english']]\n",
    "print(\"After filtering English Abstracts:\", len(df))\n",
    "\n",
    "#Clean abstract text(remove XML/HTML tags, remove non-ASCII, normalize whitespace)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text) #remove XML/HTML tags\n",
    "    text = text.encode(\"ascii\",\"ignore\").decode() #remove non-ASCII\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() #normalize whitespace\n",
    "    return text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(clean_text)\n",
    "\n",
    "#remove noice/garbage abstracts\n",
    "def is_clean_abstract(text):\n",
    "    if not text or len(text.split()) < 5:\n",
    "        return False\n",
    "    if re.match(r'^[^a-zA-Z0-9]+$', text.strip().split()[0]):\n",
    "        return False\n",
    "    total_chars = len(text)\n",
    "    punct_chars = sum(1 for c in text if c in r\"\"\"!@#$%^&*()_+[]{};:'\"\\|,<.>/?`~-=—…\"\"\")\n",
    "    if punct_chars / total_chars > 0.1:\n",
    "        return False\n",
    "    if len(re.findall(r'(,,|;;|\\.{2,}|\\-{2,})', text)) > 3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df = df[df['Abstract'].apply(is_clean_abstract)]\n",
    "print(\"After removing garbage/irregular abstracts:\", len(df))\n",
    "\n",
    "#drop rows where year is missing or non-numeric\n",
    "df = df[pd.to_numeric(df['Publication Year'], errors='coerce').notnull()]\n",
    "df['Publication Year'] = df['Publication Year'].astype(int)\n",
    "\n",
    "#filtering valid year range(1812-2025)\n",
    "df = df[df['Publication Year'].between(1812,2025,inclusive = 'both')]\n",
    "print(\"After filtering by valid year range:\",len(df))\n",
    "\n",
    "#drop duploicate abstracts \n",
    "df = df.drop_duplicates(subset = ['Abstract'])\n",
    "print(\"After removing duplicate abstracts:\", len(df))\n",
    "\n",
    "#keep only relevant coloumns \n",
    "df =  df[['Lens ID', 'Title', 'Abstract', 'Publication Year']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e52ad7-61a7-4fae-a0e3-214a92df14d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved! Final total records: 27048\n"
     ]
    }
   ],
   "source": [
    "#save cleaned dataset\n",
    "df.to_csv('C:/Users/Muditha/OneDrive - University of Eastern Finland/Documents/UEF/Thesis/AI-writing-detector/data/cleaned_dataset.csv', index=False)\n",
    "print(\"Cleaned file saved! Final total records:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969c5c9-67c6-4de0-9abe-b4bd24ec176a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
